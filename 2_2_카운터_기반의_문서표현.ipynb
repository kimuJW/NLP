{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0MBZ_OvZQPIU"
      },
      "source": [
        "# 2주차(2-2). 카운트 기반의 문서표현\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 201801544 김진원"
      ],
      "metadata": {
        "id": "P6UkURyzom6Y"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8HEzTD_eQPIW"
      },
      "source": [
        "## 1. BOW 기반의 카운트 벡터 생성"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "bhO1nZs3QPIW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c9efaa54-fc1f-44d7-ff6e-6eb4a98bca56"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package movie_reviews to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/movie_reviews.zip.\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ],
      "source": [
        "import nltk\n",
        "nltk.download('movie_reviews')\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "j4tX9_WvQPIY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bfb14808-77c4-4e02-a79a-129c8ad621e1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "#review count: 2000\n",
            "#samples of file ids: ['neg/cv000_29416.txt', 'neg/cv001_19502.txt', 'neg/cv002_17424.txt', 'neg/cv003_12683.txt', 'neg/cv004_12641.txt', 'neg/cv005_29357.txt', 'neg/cv006_17022.txt', 'neg/cv007_4992.txt', 'neg/cv008_29326.txt', 'neg/cv009_29417.txt']\n",
            "#categories of reviews: ['neg', 'pos']\n",
            "#num of \"neg\" reviews: 1000\n",
            "#num of \"pos\" reviews: 1000\n",
            "#id of the first review: neg/cv000_29416.txt\n",
            "#first review content:\n",
            " plot : two teen couples go to a church party , drink and then drive . \n",
            "they get into an accident . \n",
            "one of the guys dies , but his girlfriend continues to see him in her life , and has nightmares . \n",
            "w\n",
            "\n",
            "#sentence tokenization result: ['plot : two teen couples go to a church party , drink and then drive .', 'they get into an accident .']\n",
            "#word tokenization result: ['plot', ':', 'two', 'teen', 'couples', 'go', 'to', 'a', 'church', 'party', ',', 'drink', 'and', 'then', 'drive', '.', 'they', 'get', 'into', 'an']\n"
          ]
        }
      ],
      "source": [
        "from nltk.corpus import movie_reviews\n",
        "from nltk.tokenize import sent_tokenize, word_tokenize\n",
        "\n",
        "print('#review count:', len(movie_reviews.fileids())) #영화 리뷰 문서의 id를 반환\n",
        "print('#samples of file ids:', movie_reviews.fileids()[:10]) #id를 10개까지만 출력\n",
        "print('#categories of reviews:', movie_reviews.categories()) # label, 즉 긍정인지 부정인지에 대한 분류\n",
        "print('#num of \"neg\" reviews:', len(movie_reviews.fileids(categories='neg'))) #label이 부정인 문서들의 id를 반환\n",
        "print('#num of \"pos\" reviews:', len(movie_reviews.fileids(categories='pos'))) #label이 긍정인 문서들의 id를 반환\n",
        "fileid = movie_reviews.fileids()[0] #첫번째 문서의 id를 반환\n",
        "print('#id of the first review:', fileid)\n",
        "print('#first review content:\\n', movie_reviews.raw(fileid)[:200]) #첫번째 문서의 내용을 200자까지만 출력\n",
        "print()\n",
        "print('#sentence tokenization result:', sent_tokenize(movie_reviews.raw(fileid))[:2]) #첫번째 문서를 sentence tokenize한 결과 중 앞 두 문장\n",
        "print('#word tokenization result:', word_tokenize(movie_reviews.raw(fileid))[:20]) #첫번째 문서를 word tokenize한 결과 중 앞 스무 단어"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "FKyZJRiVQPIY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "91bafaeb-0fac-4800-c5a2-c66f017e8b3e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['plot', ':', 'two', 'teen', 'couples', 'go', 'to', 'a', 'church', 'party', ',', 'drink', 'and', 'then', 'drive', '.', 'they', 'get', 'into', 'an', 'accident', '.', 'one', 'of', 'the', 'guys', 'dies', ',', 'but', 'his', 'girlfriend', 'continues', 'to', 'see', 'him', 'in', 'her', 'life', ',', 'and', 'has', 'nightmares', '.', 'what', \"'s\", 'the', 'deal', '?', 'watch', 'the']\n"
          ]
        }
      ],
      "source": [
        "documents = [word_tokenize(movie_reviews.raw(fileid)) for fileid in movie_reviews.fileids()]\n",
        "print(documents[0][:50]) #첫째 문서의 앞 50개 단어를 출력"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###단어의 빈도수를 확인하고 싶다면 아래 코드를 실행하고"
      ],
      "metadata": {
        "id": "cUNLGTUePe8h"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "dnh2dLOoQPIY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "028b42af-257d-46f9-b205-4fed20556826"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "count of ',': 77717, count of 'the': 76276, count of '.': 65876, count of 'a': 37995, count of 'and': 35404, count of 'of': 33972, count of 'to': 31772, count of 'is': 26054, count of 'in': 21611, count of ''s': 18128, "
          ]
        }
      ],
      "source": [
        "word_count = {}\n",
        "for text in documents:\n",
        "    for word in text:\n",
        "        word_count[word] = word_count.get(word, 0) + 1\n",
        "\n",
        "sorted_features = sorted(word_count, key=word_count.get, reverse=True)\n",
        "for word in sorted_features[:10]:\n",
        "    print(f\"count of '{word}': {word_count[word]}\", end=', ')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###sorted_features의 처음 10개 까지의 단어들에 대하여 그 빈도수를 출력한다."
      ],
      "metadata": {
        "id": "GlyCZpomQRoS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* 결과를 보면 ',': 77717, 'the': 76276, '.': 65876, 'a': 37995, 'and': 35404, 'of': 33972, 'to': 31772, 'is': 26054, 'in': 21611, ''s': 18128 다음 10가지의 단어들이 이러한 빈도수를 가진 것을 확인할 수 있다."
      ],
      "metadata": {
        "id": "FGj8fZWrP8jh"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "VnCiEteiQPIZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a3cc17a7-3923-4741-fb70-06f6c700efed"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "num of features: 45953\n",
            "count of 'film': 9443, count of 'n't': 6217, count of 'movie': 5671, count of 'one': 5582, count of 'like': 3547, count of 'even': 2556, count of 'good': 2316, count of 'time': 2282, count of 'would': 2264, count of 'story': 2146, "
          ]
        }
      ],
      "source": [
        "from nltk.corpus import stopwords #일반적으로 분석대상이 아닌 단어들\n",
        "\n",
        "english_stops = set(stopwords.words('english')) #영어 불용어를 가져옴\n",
        "\n",
        "#words() 대신 raw()를 이용해 원문을 가져옴\n",
        "documents = [movie_reviews.raw(fileid) for fileid in movie_reviews.fileids()]\n",
        "\n",
        "# stopwords의 적용과 토큰화를 동시에 수행.\n",
        "tokens = [[token for token in word_tokenize(doc) if token not in english_stops and len(token) > 2] for doc in documents]\n",
        "\n",
        "word_count = {}\n",
        "for text in tokens:\n",
        "    for word in text:\n",
        "        word_count[word] = word_count.get(word, 0) + 1\n",
        "\n",
        "sorted_features = sorted(word_count, key=word_count.get, reverse=True)\n",
        "\n",
        "print('num of features:', len(sorted_features))\n",
        "for word in sorted_features[:10]:\n",
        "    print(f\"count of '{word}': {word_count[word]}\", end=', ')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "oJSgSJL6QPIZ"
      },
      "outputs": [],
      "source": [
        "word_features = sorted_features[:1000] #빈도가 높은 상위 1000개의 단어만 추출하여 features를 구성"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "0hXwhgZSQPIZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "daf8b607-ad73-4294-c11f-8809ce270de9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0, 2, 0, 1, 0]\n"
          ]
        }
      ],
      "source": [
        "def document_features(document, word_features):\n",
        "    word_count = {}\n",
        "    for word in document: #document에 있는 단어들에 대해 빈도수를 먼저 계산\n",
        "        word_count[word] = word_count.get(word, 0) + 1\n",
        "\n",
        "    features = []\n",
        "    for word in word_features: #word_features의 단어에 대해 계산된 빈도수를 feature에 추가\n",
        "        features.append(word_count.get(word, 0)) #빈도가 없는 단어는 0을 입력\n",
        "    return features\n",
        "\n",
        "word_features_ex = ['one', 'two', 'teen', 'couples', 'solo']\n",
        "doc_ex = ['two', 'two', 'couples']\n",
        "print(document_features(doc_ex, word_features_ex))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "y20O-ylCQPIa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6b88fafa-6634-42ac-ebca-432f248e4684"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(film, 6), (n't, 6), (movie, 6), (one, 3), (like, 3), (even, 3), (good, 2), (time, 0), (would, 1), (story, 0), (much, 0), (character, 2), (also, 1), (get, 3), (characters, 1), (two, 2), (first, 0), (see, 2), (way, 2), (well, 1), "
          ]
        }
      ],
      "source": [
        "feature_sets = [document_features(d, word_features) for d in tokens]\n",
        "\n",
        "# 첫째 feature set의 내용을 앞 20개만 word_features의 단어와 함께 출력\n",
        "for i in range(20):\n",
        "    print(f'({word_features[i]}, {feature_sets[0][i]})', end=', ')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "roqy5JoaQPIa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a32c692d-214e-4072-a5ea-d85b1e8aed31"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n"
          ]
        }
      ],
      "source": [
        "print(feature_sets[0][-20:])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3zNZ_IBNQPIa"
      },
      "source": [
        "## 2. 사이킷런을 이용한 카운트 벡터 생성"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "80KdYKyrQPIa"
      },
      "source": [
        "### CountVectorizer\n",
        "\n",
        "http://scikit-learn.org/stable/modules/feature_extraction.html#text-feature-extraction  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "6zgO2b5bQPIa"
      },
      "outputs": [],
      "source": [
        "# data 준비, movie_reviews.raw()를 사용하여 raw text를 추출\n",
        "reviews = [movie_reviews.raw(fileid) for fileid in movie_reviews.fileids()]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "mVYPeCi8QPIb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ebe9ea2e-6145-4176-bf09-0e0bd8505eb8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CountVectorizer(vocabulary=['film', \"n't\", 'movie', 'one', 'like', 'even',\n",
            "                            'good', 'time', 'would', 'story', 'much',\n",
            "                            'character', 'also', 'get', 'characters', 'two',\n",
            "                            'first', 'see', 'way', 'well', 'could', 'make',\n",
            "                            'really', 'films', 'little', 'life', 'plot',\n",
            "                            'people', 'scene', 'bad', ...])\n"
          ]
        }
      ],
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "#cv = CountVectorizer() #모든 매개변수에 디폴트 값을 사용하는 경우\n",
        "\n",
        "#앞에서 생성한 word_features를 이용하여 특성 집합을 지정하는 경우\n",
        "cv = CountVectorizer(vocabulary=word_features)\n",
        "\n",
        "#cv = CountVectorizer(max_features=1000) #특성 집합을 지정하지 않고 최대 특성의 수를 지정하는 경우\n",
        "print(cv) #객체에 사용된 인수들을 확인"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "qsVdPQQYQPIb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "011c81ee-0860-416b-e269-f431b24f59ad"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['film' \"n't\" 'movie' 'one' 'like' 'even' 'good' 'time' 'would' 'story'\n",
            " 'much' 'character' 'also' 'get' 'characters' 'two' 'first' 'see' 'way'\n",
            " 'well']\n",
            "['film', \"n't\", 'movie', 'one', 'like', 'even', 'good', 'time', 'would', 'story', 'much', 'character', 'also', 'get', 'characters', 'two', 'first', 'see', 'way', 'well']\n"
          ]
        }
      ],
      "source": [
        "reviews_cv = cv.fit_transform(reviews) #reviews를 이용하여 count vector를 학습하고, 변환\n",
        "print(cv.get_feature_names_out()[:20]) # count vector에 사용된 feature 이름을 반환\n",
        "print(word_features[:20]) # 비교를 위해 출력"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "p_V124NBQPIb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "13880560-69eb-4044-faec-af05063e4602"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "#type of count vectors: <class 'scipy.sparse._csr.csr_matrix'>\n",
            "#shape of count vectors: (2000, 1000)\n",
            "#sample of count vector:\n",
            "  (0, 0)\t6\n",
            "  (0, 2)\t6\n",
            "  (0, 3)\t3\n",
            "  (0, 4)\t3\n",
            "  (0, 5)\t3\n",
            "  (0, 6)\t2\n",
            "  (0, 8)\t1\n"
          ]
        }
      ],
      "source": [
        "print('#type of count vectors:', type(reviews_cv))\n",
        "print('#shape of count vectors:', reviews_cv.shape)\n",
        "print('#sample of count vector:')\n",
        "print(reviews_cv[0, :10])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "sMimZoNDQPIb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2350ea3f-3b85-43b2-8ca8-04a3d1c94662"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<2000x1000 sparse matrix of type '<class 'numpy.int64'>'\n",
              "\twith 253277 stored elements in Compressed Sparse Row format>"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ],
      "source": [
        "reviews_cv"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "1hHNMbBFQPIb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b34feaf3-a0bd-4c7c-fc48-0601fea56cf7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "film:6, n't:0, movie:6, one:3, like:3, even:3, good:2, time:0, would:1, story:0, much:0, character:2, also:1, get:3, characters:1, two:2, first:0, see:2, way:3, well:1, "
          ]
        }
      ],
      "source": [
        "for word, count in zip(cv.get_feature_names_out()[:20], reviews_cv[0].toarray()[0, :20]):\n",
        "    print(f'{word}:{count}', end=', ')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "---"
      ],
      "metadata": {
        "id": "7KcGBvN9oumi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# [[실습 2-2-1]] 마크다운 텍스트 추가하기\n",
        "## 코드 셀 위에 이를 설명하는 마크다운 텍스트를 3개 이상 추가하시오. (포매팅을 하지는 않아도 됨)\n",
        "* 코드 작동에 대한 설명 혹은 코드 결과에 대한 설명 등\n",
        "\n",
        "예시)"
      ],
      "metadata": {
        "id": "tpqUSrtNSwF0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "### 특성의 값을 단어와 함께 보고 싶다면,"
      ],
      "metadata": {
        "id": "2A513CVkohVw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for word, count in zip(cv.get_feature_names_out()[:20], reviews_cv[0].toarray()[0, :20]):\n",
        "    print(f'{word}:{count}', end=', ')"
      ],
      "metadata": {
        "id": "B-RGGdavoXLT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "03c024b1-c488-43fc-df15-f9b2a02baeb9"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "10:10, ability:0, able:0, about:2, above:0, absolutely:0, across:0, act:0, acting:0, action:0, actor:0, actors:1, actress:0, actual:0, actually:2, add:0, after:2, again:2, against:0, age:0, "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 0번째 문서에 대해 20번째 feature 까지의 값을 살펴보았다.\n",
        "* film은 6번 등장하였고, n't는 0번 등장하였다. 이처럼 feature로 사용된 단어가 몇 번 등장했는지 횟수가 담긴 matrix가 만들어졌음을 알 수 있다."
      ],
      "metadata": {
        "id": "4zQP0Wxwpcep"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---"
      ],
      "metadata": {
        "id": "edkHQ0h1_uwM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# [[실습 2-2-2]] CountVectorizer() 활용하기\n",
        "\n",
        "\n",
        "```\n",
        "cv = CountVectorizer(vocabulary=word_features)\n",
        "```\n",
        "부분을 주석처리하고,\n",
        "\n",
        "```\n",
        "cv = CountVectorizer(max_features=1000)\n",
        "```\n",
        "주석처리를 해제하여 실행한 후\n",
        "\n",
        "\n",
        "\n",
        "```\n",
        "for word, count in zip(cv.get_feature_names_out()[:20], reviews_cv[0].toarray()[0, :20]):\n",
        "    print(f'{word}:{count}', end=', ')\n",
        "```\n",
        "의 결과 비교해보기\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "q9xsMAmu_00Z"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "cv = CountVectorizer(max_features=1000)를 실행하였을 때에는 10:10, ability:0, able:0, about:2, above:0, absolutely:0, across:0, act:0, acting:0, action:0, actor:0, actors:1, actress:0, actual:0, actually:2, add:0, after:2, again:2, against:0, age:0, 이라는 결과가 나왔고\n",
        "\n",
        "cv = CountVectorizer(vocabulary=word_features)를 실행하였을 때에는 film:6, n't:0, movie:6, one:3, like:3, even:3, good:2, time:0, would:1, story:0, much:0, character:2, also:1, get:3, characters:1, two:2, first:0, see:2, way:3, well:1, 이라는 결과가 나왔다.\n",
        "\n",
        "word_feature를 이용하면 film이라는 단어의 빈도수가 6으로 가장 높았고, word_feature를 잘 만드는 것의 중요성을 알 수 있었다."
      ],
      "metadata": {
        "id": "ypAhEv-5Ot65"
      }
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.12"
    },
    "toc": {
      "base_numbering": 1,
      "nav_menu": {},
      "number_sections": true,
      "sideBar": true,
      "skip_h1_title": false,
      "title_cell": "Table of Contents",
      "title_sidebar": "Contents",
      "toc_cell": true,
      "toc_position": {},
      "toc_section_display": true,
      "toc_window_display": true
    },
    "varInspector": {
      "cols": {
        "lenName": 16,
        "lenType": 16,
        "lenVar": 40
      },
      "kernels_config": {
        "python": {
          "delete_cmd_postfix": "",
          "delete_cmd_prefix": "del ",
          "library": "var_list.py",
          "varRefreshCmd": "print(var_dic_list())"
        },
        "r": {
          "delete_cmd_postfix": ") ",
          "delete_cmd_prefix": "rm(",
          "library": "var_list.r",
          "varRefreshCmd": "cat(var_dic_list()) "
        }
      },
      "types_to_exclude": [
        "module",
        "function",
        "builtin_function_or_method",
        "instance",
        "_Feature"
      ],
      "window_display": false
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}